{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bebb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'saved_models/best_ae_64_tanh_lr1e4_50ep_AdamW.keras'  \n",
    "model = load_model(\n",
    "    model_path,\n",
    "    custom_objects={'r2': r2},\n",
    "    compile=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1832ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.AdamW(),\n",
    "    loss='mse',\n",
    "    metrics=[tf.keras.metrics.RootMeanSquaredError(), r2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6810aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "\n",
    "all_losses, threshold = evaluate_and_detect(model, test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9956f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Latent characteristics \n",
    "latents = extract_and_save_latents(model, test_ds, output_path=\"latent_features_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b381c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstruct and evaluate error \n",
    "# ————————————————————————————————————\n",
    "# 1) Configuración inicial\n",
    "# ————————————————————————————————————\n",
    "# Ruta al modelo autoencoder entrenado\n",
    "MODEL_PATH = \"saved_models/best_ae_64_tanh_lr1e4_50ep_AdamW.keras\"\n",
    "\n",
    "# ————————————————————————————————————\n",
    "# 2) Carga del modelo y definición del encoder\n",
    "# ————————————————————————————————————\n",
    "# Cargamos el AE completo (sin compilar, para acelerar)\n",
    "ae = tf.keras.models.load_model(\n",
    "    MODEL_PATH,\n",
    "    custom_objects={'r2': r2},\n",
    "    compile=False\n",
    ")\n",
    "\n",
    "for i, layer in enumerate(ae.layers):\n",
    "    name = layer.name\n",
    "    cls  = layer.__class__.__name__\n",
    "\n",
    "    try:\n",
    "        shape = layer.output.shape\n",
    "    except AttributeError:\n",
    "        # Algunas capas (InputLayer) no tienen .output, pero sí .batch_input_shape\n",
    "        shape = getattr(layer, 'batch_input_shape', 'unknown')\n",
    "\n",
    "    print(f\"{i:02d}: {name:20s} ({cls:15s}) → {shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea51973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos el encoder a partir de la capa bottleneck\n",
    "encoder = tf.keras.Model(\n",
    "    inputs=ae.input,\n",
    "    outputs=ae.get_layer(\"lstm\").output\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3967d6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Codifica tu dataset completo de ciclos de test/train/val\n",
    "latent_train = encoder.predict(train_ds, verbose=1)\n",
    "latent_val   = encoder.predict(val_ds,   verbose=1)\n",
    "latent_test  = encoder.predict(test_ds,  verbose=1)\n",
    "\n",
    "print(\"Shapes latentes:\", latent_train.shape, latent_val.shape, latent_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82470d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Índices de atributos que queremos reconstruir y medir\n",
    "ATTR_INDICES = [311, 313, 306]\n",
    "\n",
    "# Tamaño de lote para evaluación\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "test_ds_batched = (\n",
    "    test_ds\n",
    "      .batch(BATCH_SIZE)\n",
    "      .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# 4) Iteramos para reconstruir y acumular\n",
    "orig_attrs_batches = []\n",
    "recon_attrs_batches = []\n",
    "\n",
    "for batch_data, _ in test_ds_batched:\n",
    "    # 4.1 Reconstrucción completa (devuelve np.ndarray si ae.predict)\n",
    "    recon_full = ae.predict(batch_data, verbose=0)  \n",
    "    \n",
    "    # 4.2 Convertimos tensores a NumPy\n",
    "    orig_np  = batch_data.numpy()\n",
    "    recon_np = recon_full if isinstance(recon_full, np.ndarray) else recon_full.numpy()\n",
    "    \n",
    "    # 4.3 Extraemos sólo las columnas de interés\n",
    "    #     Forma: (batch_size, timesteps, n_attrs)\n",
    "    orig_sub  = orig_np[..., ATTR_INDICES]\n",
    "    recon_sub = recon_np[..., ATTR_INDICES]\n",
    "    \n",
    "    orig_attrs_batches.append(orig_sub)\n",
    "    recon_attrs_batches.append(recon_sub)\n",
    "\n",
    "# 5) Concatenamos todos los batches\n",
    "orig_attrs  = np.concatenate(orig_attrs_batches, axis=0)   # (N_total, timesteps, n_attrs)\n",
    "recon_attrs = np.concatenate(recon_attrs_batches, axis=0)  # (N_total, timesteps, n_attrs)\n",
    "\n",
    "# 6) Calculamos métricas por atributo\n",
    "#    Promedio sobre muestras y tiempo\n",
    "mse  = np.mean((recon_attrs - orig_attrs)**2,   axis=(0, 1))\n",
    "rmse = np.sqrt(mse)\n",
    "mae  = np.mean(np.abs(recon_attrs - orig_attrs), axis=(0, 1))\n",
    "\n",
    "# 7) Guardamos y mostramos resultados\n",
    "np.save(\"reconstructed_attrs.npy\", recon_attrs)\n",
    "print(\"Saved reconstructed attributes to reconstructed_attrs.npy\\n\")\n",
    "\n",
    "print(\"Error por atributo:\")\n",
    "for i, attr_idx in enumerate(ATTR_INDICES):\n",
    "    print(f\"  Atributo {attr_idx}:  MSE={mse[i]:.6f},  RMSE={rmse[i]:.6f},  MAE={mae[i]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0d3430",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_and_save_latents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m latents \u001b[38;5;241m=\u001b[39m \u001b[43mextract_and_save_latents\u001b[49m(model, test_ds, output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatent_features_test.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m labels \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfit_predict(latents)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extract_and_save_latents' is not defined"
     ]
    }
   ],
   "source": [
    "#Clustering \n",
    "\n",
    "latents = extract_and_save_latents(model, test_ds, output_path=\"latent_features_test.npy\")\n",
    "labels = KMeans(n_clusters=2).fit_predict(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd59ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inv_Di",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
