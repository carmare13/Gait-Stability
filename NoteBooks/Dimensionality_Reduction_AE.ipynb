{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2560fba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 10:31:11.285648: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-16 10:31:11.304294: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-16 10:31:11.308855: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-16 10:31:11.321453: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-16 10:31:12.063022: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed precision enabled\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys \n",
    "os.chdir('..')\n",
    "sys.path.insert(0, os.getcwd())\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K \n",
    "import glob\n",
    "\n",
    "from Data_loader import load_subjects_from_json, get_all_npy_paths_by_group, base_folders\n",
    "from AE_pipeline import (\n",
    "    convert_npy_to_tfrecord,\n",
    "    create_tfrecord_dataset,\n",
    "    write_sharded_tfrecord,\n",
    "    make_monolithic_ds,\n",
    "    build_lstm_autoencoder,\n",
    "    train_autoencoder,\n",
    "    evaluate_and_detect,\n",
    "    extract_and_save_latents,\n",
    "    n_timesteps,\n",
    "    NUM_BIOMECHANICAL_VARIABLES,\n",
    "    _parse_cycle,\n",
    "    BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af456de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load subjects lists \n",
    "train_subjects = {\n",
    "    \"G01\": load_subjects_from_json(\"G01_train_subjects.json\"),\n",
    "    \"G03\": load_subjects_from_json(\"G03_train_subjects.json\")\n",
    "}\n",
    "val_subjects = {\n",
    "    \"G01\": load_subjects_from_json(\"G01_validation_subjects.json\"),\n",
    "    \"G03\": load_subjects_from_json(\"G03_validation_subjects.json\")\n",
    "}\n",
    "test_subjects = {\n",
    "    \"G01\": load_subjects_from_json(\"G01_test_subjects.json\"),\n",
    "    \"G03\": load_subjects_from_json(\"G03_test_subjects.json\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "379bf567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S030/preprocessed/S030_D01_B01_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S030/preprocessed/S030_D01_B02_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S030/preprocessed/S030_D01_B02_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S030/preprocessed/S030_D01_B02_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S030/preprocessed/S030_D01_B03_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B01_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B01_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B01_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B02_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B02_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B02_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B03_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B03_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S012/preprocessed/S012_D01_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S014/preprocessed/S014_D02_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S033/preprocessed/S033_D02_B01_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S033/preprocessed/S033_D02_B02_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S031/preprocessed/S031_D01_B01_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S031/preprocessed/S031_D02_B01_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S031/preprocessed/S031_D02_B02_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S031/preprocessed/S031_D02_B02_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S031/preprocessed/S031_D02_B03_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S031/preprocessed/S031_D02_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S039/preprocessed/S039_D01_B01_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S039/preprocessed/S039_D01_B02_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S039/preprocessed/S039_D01_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S034/preprocessed/S034_D01_B01_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S034/preprocessed/S034_D01_B02_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S034/preprocessed/S034_D01_B02_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S034/preprocessed/S034_D01_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B01_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B01_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B01_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B02_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B02_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B02_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B03_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B03_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S005/preprocessed/S005_D02_B01_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S019/preprocessed/S019_D01_B03_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S107/preprocessed/S107_D01_B03_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D01_B01_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B01_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B01_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B01_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B02_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B02_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B02_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B03_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B03_T03_preprocessed.npy\n",
      "Train .npy: 932 files\n",
      " Val  .npy: 196 files\n",
      " Test .npy: 169 files\n"
     ]
    }
   ],
   "source": [
    "# Generate routes .npy\n",
    "train_npy = get_all_npy_paths_by_group(train_subjects, base_folders)\n",
    "val_npy   = get_all_npy_paths_by_group(val_subjects,   base_folders)\n",
    "test_npy  = get_all_npy_paths_by_group(test_subjects,  base_folders)\n",
    "\n",
    "print(f\"Train .npy: {len(train_npy)} files\")\n",
    "print(f\" Val  .npy: {len(val_npy)} files\")\n",
    "print(f\" Test .npy: {len(test_npy)} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0e7a263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Shards ya existen en: train_shards/\n"
     ]
    }
   ],
   "source": [
    "#Generar SHARDS para TRAIN \n",
    "shards_dir = \"train_shards\"\n",
    "if not os.path.isdir(shards_dir):\n",
    "    write_sharded_tfrecord(\n",
    "        npy_paths=train_npy,\n",
    "        output_dir=shards_dir,\n",
    "        shard_size=5_000\n",
    "    )\n",
    "    print(f\"→ Shards generados en: {shards_dir}/\")\n",
    "else:\n",
    "    print(f\"→ Shards ya existen en: {shards_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa0562cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping (already exists) → val_cycles.tfrecord.gz\n",
      "Skipping (already exists) → test_cycles.tfrecord.gz\n"
     ]
    }
   ],
   "source": [
    "#Convertir VAL y TEST a TFRecord monolítico \n",
    "for split, npy_list in [(\"val\", val_npy), (\"test\", test_npy)]:\n",
    "    tfp = f\"{split}_cycles.tfrecord.gz\"\n",
    "    if not os.path.exists(tfp):\n",
    "        convert_npy_to_tfrecord(npy_list, tfp)\n",
    "        print(f\"Converted → {tfp}\")\n",
    "    else:\n",
    "        print(f\"Skipping (already exists) → {tfp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ac1279c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1750080679.836742  770394 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750080679.876470  770394 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750080679.876659  770394 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750080679.877612  770394 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750080679.877763  770394 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750080679.877893  770394 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750080679.955277  770394 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750080679.955493  770394 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750080679.955639  770394 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-06-16 10:31:19.955742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6394 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ train_ds: <_PrefetchDataset element_spec=(TensorSpec(shape=(32, 100, 321), dtype=tf.float32, name=None), TensorSpec(shape=(32, 100, 321), dtype=tf.float32, name=None))>\n",
      "→ val_ds:   <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 100, 321), dtype=tf.float32, name=None), TensorSpec(shape=(None, 100, 321), dtype=tf.float32, name=None))>\n",
      "→ test_ds:  <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 100, 321), dtype=tf.float32, name=None), TensorSpec(shape=(None, 100, 321), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "#Create tf.data.Dataset\n",
    "# 3a) Lista de archivos shard\n",
    "shard_files = sorted(glob.glob(os.path.join(shards_dir, \"*.tfrecord.gz\")))\n",
    "\n",
    "# 3b) Pipeline shard-aware\n",
    "train_ds = (\n",
    "   tf.data.Dataset\n",
    "      .list_files(shard_files, shuffle=True)\n",
    "      .interleave(\n",
    "         lambda f: tf.data.TFRecordDataset(f, compression_type=\"GZIP\"),\n",
    "         cycle_length=4,\n",
    "         num_parallel_calls=tf.data.AUTOTUNE\n",
    "      )\n",
    "      .map(_parse_cycle, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "      .shuffle(5_000, seed=42)\n",
    "      .batch(BATCH_SIZE, drop_remainder=True)\n",
    "      .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# VAL y TEST: dataset monolítico\n",
    "\n",
    "\n",
    "val_ds  = make_monolithic_ds(\"val_cycles.tfrecord.gz\")\n",
    "test_ds = make_monolithic_ds(\"test_cycles.tfrecord.gz\")\n",
    "\n",
    "print(f\"→ train_ds: {train_ds}\")\n",
    "print(f\"→ val_ds:   {val_ds}\")\n",
    "print(f\"→ test_ds:  {test_ds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f3d333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input  ◂ min: -12.165039 max: 15.267122 mean: -0.009751429 std: 1.0221982\n",
      "Target ◂ -12.165039 15.267122\n",
      "Any NaN in x? False\n",
      "Any Inf in x? False\n"
     ]
    }
   ],
   "source": [
    "# Optional from a batch validate if still NaN \n",
    "for x_batch, y_batch in train_ds.take(1):\n",
    "      import tensorflow as tf\n",
    "      print(\"Input  ◂ min:\", tf.reduce_min(x_batch).numpy(),\n",
    "            \"max:\", tf.reduce_max(x_batch).numpy(),\n",
    "            \"mean:\", tf.reduce_mean(x_batch).numpy(),\n",
    "            \"std:\", tf.math.reduce_std(x_batch).numpy())\n",
    "      print(\"Target ◂\", \n",
    "            tf.reduce_min(y_batch).numpy(), tf.reduce_max(y_batch).numpy())\n",
    "      # Comprueba si hay NaN/Inf\n",
    "      print(\"Any NaN in x?\", tf.reduce_any(tf.math.is_nan(x_batch)).numpy())\n",
    "      print(\"Any Inf in x?\", tf.reduce_any(tf.math.is_inf(x_batch)).numpy())\n",
    "      break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f0aef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Ejecutando experimento: 16_adamw_lr1e5_50ep ----\n",
      "Epoch 1/30\n",
      "   1726/Unknown \u001b[1m364s\u001b[0m 206ms/step - loss: 1.9919 - r2: 0.0038 - root_mean_squared_error: 1.0806"
     ]
    }
   ],
   "source": [
    "#Build and train the Autoencoder\n",
    "# Hiperparameters \n",
    "def r2(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    ss_total = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    ss_residual = K.sum(K.square(y_true - y_pred))\n",
    "    return 1 - (ss_residual / ss_total)\n",
    "\n",
    "optimizers = {\n",
    "    #\"adam\": tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    \"adamw\": tf.keras.optimizers.AdamW(learning_rate=1e-5),\n",
    "    #\"sgd\": tf.keras.optimizers.SGD(learning_rate=1e-4)\n",
    "}\n",
    "\n",
    "\n",
    "#run_id = \"32_tanh_lr1e4_50ep\"\n",
    "n_timesteps = 100\n",
    "n_vars = 321\n",
    "latent_dim = 16\n",
    "enc_activation = 'tanh'\n",
    "dec_activation = 'tanh'\n",
    "dense_activation = 'linear'\n",
    "recurrent_activation = 'sigmoid'\n",
    "dropout = 0.2\n",
    "recurrent_dropout = 0.2\n",
    "lr = 1e-5\n",
    "epochs = 30\n",
    "\n",
    "all_histories = {}\n",
    "\n",
    "\n",
    "for opt_name, optimizer in optimizers.items():\n",
    "    run_id = f\"16_{opt_name}_lr1e5_50ep\"\n",
    "    print(f\"---- Ejecutando experimento: {run_id} ----\")\n",
    "    model = build_lstm_autoencoder(\n",
    "        n_timesteps=n_timesteps,\n",
    "        n_vars=n_vars,\n",
    "        latent_dim=latent_dim,\n",
    "        enc_activation=enc_activation,\n",
    "        dec_activation=dec_activation,\n",
    "        dense_activation=dense_activation,\n",
    "        recurrent_activation=recurrent_activation,\n",
    "        dropout=dropout,\n",
    "        recurrent_dropout=recurrent_dropout,\n",
    "        lr=1e-4  \n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer, \n",
    "        loss='mse', \n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError(), r2]  # Usamos la función personalizada r2 aquí\n",
    "    )\n",
    "\n",
    "    history = train_autoencoder(model,\n",
    "        train_ds,\n",
    "        val_ds,\n",
    "        run_id,\n",
    "        epochs\n",
    "    )\n",
    "    all_histories[run_id] = history.history\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49af28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'saved_models/best_ae_32_tanh_lr1e4_50ep.keras'  \n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5ec9966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 11:21:33.181680: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "/home/dmartinez/miniconda3/envs/inv_Di/lib/python3.10/site-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test reconstruction MSE: 0.583976\n",
      "Detected 4667 anomalies out of 35115 (threshold=0.871904)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 11:22:27.171544: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "all_losses, threshold = evaluate_and_detect(model, test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Latent characteristics \n",
    "latents = extract_and_save_latents(model, test_ds, output_path=\"latent_features_test.npy\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inv_Di",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
