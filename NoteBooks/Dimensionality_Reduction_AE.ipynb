{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2560fba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 22:30:44.110441: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-16 22:30:44.125934: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-16 22:30:44.132224: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-16 22:30:44.144418: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-16 22:30:44.905976: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed precision enabled\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys \n",
    "os.chdir('..')\n",
    "sys.path.insert(0, os.getcwd())\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K \n",
    "import glob\n",
    "\n",
    "from Data_loader import load_subjects_from_json, get_all_npy_paths_by_group, base_folders\n",
    "from AE_pipeline import (\n",
    "    convert_npy_to_tfrecord,\n",
    "    create_tfrecord_dataset,\n",
    "    write_sharded_tfrecord,\n",
    "    make_monolithic_ds,\n",
    "    build_lstm_autoencoder,\n",
    "    train_autoencoder,\n",
    "    evaluate_and_detect,\n",
    "    extract_and_save_latents,\n",
    "    n_timesteps,\n",
    "    NUM_BIOMECHANICAL_VARIABLES,\n",
    "    _parse_cycle,\n",
    "    BATCH_SIZE,\n",
    "    reconstruct_and_evaluate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af456de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load subjects lists \n",
    "train_subjects = {\n",
    "    \"G01\": load_subjects_from_json(\"G01_train_subjects.json\"),\n",
    "    \"G03\": load_subjects_from_json(\"G03_train_subjects.json\")\n",
    "}\n",
    "val_subjects = {\n",
    "    \"G01\": load_subjects_from_json(\"G01_validation_subjects.json\"),\n",
    "    \"G03\": load_subjects_from_json(\"G03_validation_subjects.json\")\n",
    "}\n",
    "test_subjects = {\n",
    "    \"G01\": load_subjects_from_json(\"G01_test_subjects.json\"),\n",
    "    \"G03\": load_subjects_from_json(\"G03_test_subjects.json\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "379bf567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S030/preprocessed/S030_D01_B01_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S030/preprocessed/S030_D01_B02_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S030/preprocessed/S030_D01_B02_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S030/preprocessed/S030_D01_B02_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S030/preprocessed/S030_D01_B03_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B01_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B01_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B01_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B02_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B02_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B02_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B03_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B03_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S012/preprocessed/S012_D01_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S014/preprocessed/S014_D02_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S033/preprocessed/S033_D02_B01_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S033/preprocessed/S033_D02_B02_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S031/preprocessed/S031_D01_B01_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S031/preprocessed/S031_D02_B01_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S031/preprocessed/S031_D02_B02_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S031/preprocessed/S031_D02_B02_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S031/preprocessed/S031_D02_B03_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S031/preprocessed/S031_D02_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S039/preprocessed/S039_D01_B01_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S039/preprocessed/S039_D01_B02_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S039/preprocessed/S039_D01_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S034/preprocessed/S034_D01_B01_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S034/preprocessed/S034_D01_B02_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S034/preprocessed/S034_D01_B02_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S034/preprocessed/S034_D01_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B01_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B01_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B01_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B02_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B02_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B02_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B03_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B03_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S005/preprocessed/S005_D02_B01_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S019/preprocessed/S019_D01_B03_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S107/preprocessed/S107_D01_B03_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D01_B01_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B01_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B01_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B01_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B02_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B02_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B02_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B03_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B03_T03_preprocessed.npy\n",
      "Train .npy: 932 files\n",
      " Val  .npy: 196 files\n",
      " Test .npy: 169 files\n"
     ]
    }
   ],
   "source": [
    "# Generate routes .npy\n",
    "train_npy = get_all_npy_paths_by_group(train_subjects, base_folders)\n",
    "val_npy   = get_all_npy_paths_by_group(val_subjects,   base_folders)\n",
    "test_npy  = get_all_npy_paths_by_group(test_subjects,  base_folders)\n",
    "\n",
    "print(f\"Train .npy: {len(train_npy)} files\")\n",
    "print(f\" Val  .npy: {len(val_npy)} files\")\n",
    "print(f\" Test .npy: {len(test_npy)} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0e7a263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Shards ya existen en: train_shards/\n"
     ]
    }
   ],
   "source": [
    "#Generar SHARDS para TRAIN \n",
    "shards_dir = \"train_shards\"\n",
    "if not os.path.isdir(shards_dir):\n",
    "    write_sharded_tfrecord(\n",
    "        npy_paths=train_npy,\n",
    "        output_dir=shards_dir,\n",
    "        shard_size=5_000\n",
    "    )\n",
    "    print(f\"→ Shards generados en: {shards_dir}/\")\n",
    "else:\n",
    "    print(f\"→ Shards ya existen en: {shards_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa0562cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping (already exists) → val_cycles.tfrecord.gz\n",
      "Skipping (already exists) → test_cycles.tfrecord.gz\n"
     ]
    }
   ],
   "source": [
    "#Convertir VAL y TEST a TFRecord monolítico \n",
    "for split, npy_list in [(\"val\", val_npy), (\"test\", test_npy)]:\n",
    "    tfp = f\"{split}_cycles.tfrecord.gz\"\n",
    "    if not os.path.exists(tfp):\n",
    "        convert_npy_to_tfrecord(npy_list, tfp)\n",
    "        print(f\"Converted → {tfp}\")\n",
    "    else:\n",
    "        print(f\"Skipping (already exists) → {tfp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ac1279c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1750123852.482513  828159 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750123852.541680  828159 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750123852.541940  828159 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750123852.543325  828159 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750123852.543894  828159 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750123852.544095  828159 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750123852.628798  828159 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750123852.629073  828159 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750123852.629281  828159 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-06-16 22:30:52.629430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6419 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ train_ds: <_PrefetchDataset element_spec=(TensorSpec(shape=(32, 100, 321), dtype=tf.float32, name=None), TensorSpec(shape=(32, 100, 321), dtype=tf.float32, name=None))>\n",
      "→ val_ds:   <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 100, 321), dtype=tf.float32, name=None), TensorSpec(shape=(None, 100, 321), dtype=tf.float32, name=None))>\n",
      "→ test_ds:  <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 100, 321), dtype=tf.float32, name=None), TensorSpec(shape=(None, 100, 321), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "#Create tf.data.Dataset\n",
    "# 3a) Lista de archivos shard\n",
    "shard_files = sorted(glob.glob(os.path.join(shards_dir, \"*.tfrecord.gz\")))\n",
    "\n",
    "# 3b) Pipeline shard-aware\n",
    "train_ds = (\n",
    "   tf.data.Dataset\n",
    "      .list_files(shard_files, shuffle=True)\n",
    "      .interleave(\n",
    "         lambda f: tf.data.TFRecordDataset(f, compression_type=\"GZIP\"),\n",
    "         cycle_length=4,\n",
    "         num_parallel_calls=tf.data.AUTOTUNE\n",
    "      )\n",
    "      .map(_parse_cycle, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "      .shuffle(5_000, seed=42)\n",
    "      .batch(BATCH_SIZE, drop_remainder=True)\n",
    "      .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# VAL y TEST: dataset monolítico\n",
    "\n",
    "\n",
    "val_ds  = make_monolithic_ds(\"val_cycles.tfrecord.gz\")\n",
    "test_ds = make_monolithic_ds(\"test_cycles.tfrecord.gz\")\n",
    "\n",
    "print(f\"→ train_ds: {train_ds}\")\n",
    "print(f\"→ val_ds:   {val_ds}\")\n",
    "print(f\"→ test_ds:  {test_ds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f3d333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input  ◂ min: -12.165039 max: 15.267122 mean: -0.009751429 std: 1.0221982\n",
      "Target ◂ -12.165039 15.267122\n",
      "Any NaN in x? False\n",
      "Any Inf in x? False\n"
     ]
    }
   ],
   "source": [
    "# Optional from a batch validate if still NaN \n",
    "for x_batch, y_batch in train_ds.take(1):\n",
    "      import tensorflow as tf\n",
    "      print(\"Input  ◂ min:\", tf.reduce_min(x_batch).numpy(),\n",
    "            \"max:\", tf.reduce_max(x_batch).numpy(),\n",
    "            \"mean:\", tf.reduce_mean(x_batch).numpy(),\n",
    "            \"std:\", tf.math.reduce_std(x_batch).numpy())\n",
    "      print(\"Target ◂\", \n",
    "            tf.reduce_min(y_batch).numpy(), tf.reduce_max(y_batch).numpy())\n",
    "      # Comprueba si hay NaN/Inf\n",
    "      print(\"Any NaN in x?\", tf.reduce_any(tf.math.is_nan(x_batch)).numpy())\n",
    "      print(\"Any Inf in x?\", tf.reduce_any(tf.math.is_inf(x_batch)).numpy())\n",
    "      break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f0aef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Ejecutando experimento: 16_adamw_lr1e5_30ep_v2 ----\n",
      "Epoch 1/30\n",
      "   6086/Unknown \u001b[1m1190s\u001b[0m 194ms/step - loss: 1.0759 - r2: 0.0386 - root_mean_squared_error: 1.0086"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 22:50:48.096092: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2025-06-16 22:50:48.096126: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n",
      "/home/dmartinez/miniconda3/envs/inv_Di/lib/python3.10/site-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1287s\u001b[0m 210ms/step - loss: 1.0759 - r2: 0.0386 - root_mean_squared_error: 1.0086 - val_loss: 0.8998 - val_r2: 0.2140 - val_root_mean_squared_error: 0.9298 - learning_rate: 9.8000e-05\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 22:52:24.475318: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[StatefulPartitionedCall/Shape/_6]]\n",
      "2025-06-16 22:52:24.475356: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 11566264426922050253\n",
      "2025-06-16 22:52:24.475365: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 16431053499855774549\n",
      "2025-06-16 22:52:24.475373: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 13316580328375828838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1267s\u001b[0m 208ms/step - loss: 0.7566 - r2: 0.2596 - root_mean_squared_error: 0.8451 - val_loss: 0.8433 - val_r2: 0.2839 - val_root_mean_squared_error: 0.8910 - learning_rate: 9.6040e-05\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 23:13:31.643155: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[StatefulPartitionedCall/Shape/_6]]\n",
      "2025-06-16 23:13:31.643185: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 11566264426922050253\n",
      "2025-06-16 23:13:31.643194: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 16431053499855774549\n",
      "2025-06-16 23:13:31.643202: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 13316580328375828838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1284s\u001b[0m 211ms/step - loss: 0.7446 - r2: 0.3174 - root_mean_squared_error: 0.8328 - val_loss: 0.8224 - val_r2: 0.3087 - val_root_mean_squared_error: 0.8760 - learning_rate: 9.4119e-05\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 23:34:55.349340: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 11566264426922050253\n",
      "2025-06-16 23:34:55.349385: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 16431053499855774549\n",
      "2025-06-16 23:34:55.349394: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 13316580328375828838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1302s\u001b[0m 214ms/step - loss: 0.7534 - r2: 0.3476 - root_mean_squared_error: 0.8344 - val_loss: 0.8185 - val_r2: 0.3098 - val_root_mean_squared_error: 0.8766 - learning_rate: 9.2237e-05\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-16 23:56:36.927741: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[StatefulPartitionedCall/Shape/_6]]\n",
      "2025-06-16 23:56:36.927774: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 11566264426922050253\n",
      "2025-06-16 23:56:36.927783: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 16431053499855774549\n",
      "2025-06-16 23:56:36.927791: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 13316580328375828838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - loss: 0.7345 - r2: 0.3577 - root_mean_squared_error: 0.8242"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 00:16:32.998437: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 78429404853834672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1293s\u001b[0m 212ms/step - loss: 0.7344 - r2: 0.3577 - root_mean_squared_error: 0.8242 - val_loss: 0.8168 - val_r2: 0.3116 - val_root_mean_squared_error: 0.8746 - learning_rate: 8.8584e-05\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 00:18:10.267732: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 11566264426922050253\n",
      "2025-06-17 00:18:10.267797: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 16431053499855774549\n",
      "2025-06-17 00:18:10.267811: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 13316580328375828838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1290s\u001b[0m 212ms/step - loss: 0.6657 - r2: 0.3617 - root_mean_squared_error: 0.7842 - val_loss: 0.8262 - val_r2: 0.3038 - val_root_mean_squared_error: 0.8775 - learning_rate: 8.6813e-05\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 00:39:40.601690: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 11566264426922050253\n",
      "2025-06-17 00:39:40.601720: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 16431053499855774549\n",
      "2025-06-17 00:39:40.601730: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 13316580328375828838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - loss: 0.7052 - r2: 0.3554 - root_mean_squared_error: 0.8081"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 00:59:33.398799: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 78429404853834672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1291s\u001b[0m 212ms/step - loss: 0.7052 - r2: 0.3554 - root_mean_squared_error: 0.8081 - val_loss: 0.8089 - val_r2: 0.3194 - val_root_mean_squared_error: 0.8700 - learning_rate: 8.5076e-05\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 01:01:11.567627: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 11566264426922050253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1293s\u001b[0m 212ms/step - loss: 0.6194 - r2: 0.3713 - root_mean_squared_error: 0.7549 - val_loss: 0.8073 - val_r2: 0.3220 - val_root_mean_squared_error: 0.8682 - learning_rate: 8.3375e-05\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 01:22:44.147188: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[StatefulPartitionedCall/Shape/_6]]\n",
      "2025-06-17 01:22:44.147221: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 11566264426922050253\n",
      "2025-06-17 01:22:44.147230: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 16431053499855774549\n",
      "2025-06-17 01:22:44.147238: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 13316580328375828838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1293s\u001b[0m 212ms/step - loss: 0.6826 - r2: 0.3688 - root_mean_squared_error: 0.7943 - val_loss: 0.8058 - val_r2: 0.3228 - val_root_mean_squared_error: 0.8689 - learning_rate: 8.1707e-05\n",
      "Epoch 10/30\n",
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1294s\u001b[0m 212ms/step - loss: 0.6751 - r2: 0.3711 - root_mean_squared_error: 0.7904 - val_loss: 0.8005 - val_r2: 0.3267 - val_root_mean_squared_error: 0.8665 - learning_rate: 7.8472e-05\n",
      "Epoch 11/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 02:05:51.119753: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 11566264426922050253\n",
      "2025-06-17 02:05:51.119790: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 16431053499855774549\n",
      "2025-06-17 02:05:51.119800: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 13316580328375828838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1295s\u001b[0m 213ms/step - loss: 0.7231 - r2: 0.3735 - root_mean_squared_error: 0.8181 - val_loss: 0.8095 - val_r2: 0.3179 - val_root_mean_squared_error: 0.8732 - learning_rate: 7.6902e-05\n",
      "Epoch 12/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 02:27:25.798735: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 11566264426922050253\n",
      "2025-06-17 02:27:25.798770: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 16431053499855774549\n",
      "2025-06-17 02:27:25.798797: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 13316580328375828838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1296s\u001b[0m 213ms/step - loss: 0.6638 - r2: 0.3657 - root_mean_squared_error: 0.7846 - val_loss: 0.8127 - val_r2: 0.3159 - val_root_mean_squared_error: 0.8694 - learning_rate: 7.5364e-05\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 02:49:01.969347: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 11566264426922050253\n",
      "2025-06-17 02:49:01.969392: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 16431053499855774549\n",
      "2025-06-17 02:49:01.969401: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 13316580328375828838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1299s\u001b[0m 213ms/step - loss: 0.6856 - r2: 0.3779 - root_mean_squared_error: 0.7955 - val_loss: 0.8009 - val_r2: 0.3274 - val_root_mean_squared_error: 0.8677 - learning_rate: 7.3857e-05\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 03:10:40.508670: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 11566264426922050253\n",
      "2025-06-17 03:10:40.508699: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 16431053499855774549\n",
      "2025-06-17 03:10:40.508708: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 13316580328375828838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1299s\u001b[0m 213ms/step - loss: 0.7050 - r2: 0.3665 - root_mean_squared_error: 0.8087 - val_loss: 0.8188 - val_r2: 0.3103 - val_root_mean_squared_error: 0.8728 - learning_rate: 7.0932e-05\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 03:32:19.957438: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 11566264426922050253\n",
      "2025-06-17 03:32:19.957474: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 16431053499855774549\n",
      "2025-06-17 03:32:19.957487: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 13316580328375828838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6086/6086\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - loss: 0.7154 - r2: 0.3753 - root_mean_squared_error: 0.8135"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 03:52:21.524168: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 14819651930956861841\n",
      "2025-06-17 03:52:21.524204: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 78429404853834672\n",
      "2025-06-17 03:54:00.013698: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 11566264426922050253\n",
      "2025-06-17 03:54:00.013726: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 16431053499855774549\n",
      "2025-06-17 03:54:00.013736: I tensorflow/core/framework/local_rendezvous.cc:423] Local rendezvous recv item cancelled. Key hash: 13316580328375828838\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "This optimizer was created with a `LearningRateSchedule` object as its `learning_rate` constructor argument, hence its learning rate is not settable. If you need the learning rate to be settable, you should instantiate the optimizer with a float `learning_rate` argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 60\u001b[0m\n\u001b[1;32m     41\u001b[0m model \u001b[38;5;241m=\u001b[39m build_lstm_autoencoder(\n\u001b[1;32m     42\u001b[0m     n_timesteps\u001b[38;5;241m=\u001b[39mn_timesteps,\n\u001b[1;32m     43\u001b[0m     n_vars\u001b[38;5;241m=\u001b[39mn_vars,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m  \n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     54\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     55\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer, \n\u001b[1;32m     56\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     57\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mRootMeanSquaredError(), r2] \n\u001b[1;32m     58\u001b[0m )\n\u001b[0;32m---> 60\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m all_histories[run_id] \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory\n",
      "File \u001b[0;32m~/Gait-Stability/AE_pipeline.py:277\u001b[0m, in \u001b[0;36mtrain_autoencoder\u001b[0;34m(model, train_ds, val_ds, run_id, epochs, model_dir, log_dir_root, csv_log_root, lr_initial, lr_decay_rate, lr_decay_steps, optimizer)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compile_needed:\n\u001b[1;32m    271\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m    272\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m    273\u001b[0m         loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    274\u001b[0m         metrics\u001b[38;5;241m=\u001b[39m[tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mRootMeanSquaredError(), r2]\n\u001b[1;32m    275\u001b[0m     )\n\u001b[0;32m--> 277\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m model\u001b[38;5;241m.\u001b[39msave(final_model) \n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/miniconda3/envs/inv_Di/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/inv_Di/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:717\u001b[0m, in \u001b[0;36mBaseOptimizer.learning_rate\u001b[0;34m(self, learning_rate)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    715\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learning_rate, learning_rate_schedule\u001b[38;5;241m.\u001b[39mLearningRateSchedule\n\u001b[1;32m    716\u001b[0m     ):\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    718\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis optimizer was created with a `LearningRateSchedule`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    719\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m object as its `learning_rate` constructor argument, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    720\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhence its learning rate is not settable. If you need the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    721\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m learning rate to be settable, you should instantiate \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe optimizer with a float `learning_rate` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    723\u001b[0m         )\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learning_rate\u001b[38;5;241m.\u001b[39massign(learning_rate)\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev_lr_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    726\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learning_rate, backend\u001b[38;5;241m.\u001b[39mVariable\n\u001b[1;32m    727\u001b[0m ):\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;66;03m# Untrack learning rate variable\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: This optimizer was created with a `LearningRateSchedule` object as its `learning_rate` constructor argument, hence its learning rate is not settable. If you need the learning rate to be settable, you should instantiate the optimizer with a float `learning_rate` argument."
     ]
    }
   ],
   "source": [
    "#Build and train the Autoencoder\n",
    "# Hiperparameters \n",
    "def r2(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    ss_total = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    ss_residual = K.sum(K.square(y_true - y_pred))\n",
    "    return 1 - (ss_residual / ss_total)\n",
    "\n",
    "run_id = \"16_tanh_lr1e4_30ep_AdamW\"\n",
    "n_timesteps = 100\n",
    "n_vars = 321\n",
    "latent_dim = 16\n",
    "enc_activation = 'tanh'\n",
    "dec_activation = 'tanh'\n",
    "dense_activation = 'linear'\n",
    "recurrent_activation = 'sigmoid'\n",
    "dropout = 0.1\n",
    "recurrent_dropout = 0.1\n",
    "epochs = 30\n",
    "model_dir='saved_models',\n",
    "log_dir_root='logs/fit',\n",
    "csv_log_root='training_log',\n",
    "lr_initial=1e-4,\n",
    "lr_decay_rate=0.98,\n",
    "lr_decay_steps=5000 \n",
    "\n",
    "all_histories = {}\n",
    "\n",
    "optimizer = {\n",
    "    \"adamw\": tf.keras.optimizers.AdamW(learning_rate=1e-5),\n",
    "}\n",
    "\n",
    "print(f\"---- Ejecutando experimento: {run_id} ----\")\n",
    "model = build_lstm_autoencoder(\n",
    "        n_timesteps=n_timesteps,\n",
    "        n_vars=n_vars,\n",
    "        latent_dim=latent_dim,\n",
    "        enc_activation=enc_activation,\n",
    "        dec_activation=dec_activation,\n",
    "        dense_activation=dense_activation,\n",
    "        recurrent_activation=recurrent_activation,\n",
    "        dropout=dropout,\n",
    "        recurrent_dropout=recurrent_dropout\n",
    "    )\n",
    "\n",
    "model.compile(\n",
    "        optimizer=optimizer, \n",
    "        loss='mse', \n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError(), r2] \n",
    "    )\n",
    "\n",
    "history = train_autoencoder(model,\n",
    "        train_ds,\n",
    "        val_ds,\n",
    "        run_id,\n",
    "        epochs,\n",
    "    )\n",
    "all_histories[run_id] = history.history\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'saved_models/best_ae_16_adamw_50ep.keras'  \n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5ec9966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 11:21:33.181680: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "/home/dmartinez/miniconda3/envs/inv_Di/lib/python3.10/site-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test reconstruction MSE: 0.583976\n",
      "Detected 4667 anomalies out of 35115 (threshold=0.871904)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 11:22:27.171544: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "all_losses, threshold = evaluate_and_detect(model, test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Latent characteristics \n",
    "latents = extract_and_save_latents(model, test_ds, output_path=\"latent_features_test.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b74e259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstruct from latent and measure error \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from reconstruct_module import reconstruct_and_evaluate  # ajusta al nombre del módulo\n",
    "\n",
    "# 1) Usamos test_ds en lugar de cargar un archivo .npy\n",
    "#  - Ya tienes test_ds previamente cargado, solo usas ese dataset\n",
    "\n",
    "# Define los índices de las variables que te interesan\n",
    "attrs = [0, 10, 50]\n",
    "\n",
    "# Crear un Dataset iterador para obtener los datos de test en batches\n",
    "# Usamos un batch size más pequeño para evaluar (si lo necesitas)\n",
    "batch_size = 32\n",
    "\n",
    "# Recorremos el dataset por lotes\n",
    "all_metrics = []\n",
    "all_recon = []\n",
    "\n",
    "for batch_data, _ in test_ds.take(-1):  # Recorre todo el test_ds\n",
    "    batch_data = batch_data.numpy()  # Convertir de Tensor a NumPy array si es necesario\n",
    "\n",
    "    # Llama a la función de reconstrucción y evaluación\n",
    "    metrics, recon_subset = reconstruct_and_evaluate(\n",
    "        model_path=\"saved_models/ae_lstm_experiment42.keras\",\n",
    "        data=batch_data,  # Aquí pasamos cada batch del test_ds\n",
    "        attr_idx=attrs,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    all_metrics.append(metrics)  # Guarda los resultados\n",
    "    all_recon.append(recon_subset)  # Guarda las reconstrucciones\n",
    "\n",
    "# Concatenar resultados de todas las épocas\n",
    "all_metrics = {key: np.concatenate([m[key] for m in all_metrics], axis=0) for key in all_metrics[0].keys()}\n",
    "all_recon = np.concatenate(all_recon, axis=0)\n",
    "\n",
    "# 2) Guarda la reconstrucción y muestra los errores\n",
    "np.save(\"reconstructed_attrs.npy\", all_recon)\n",
    "\n",
    "print(\"Reconstructions saved to reconstructed_attrs.npy\\n\")\n",
    "print(\"Error por atributo:\")\n",
    "for i, idx in enumerate(attrs):\n",
    "    print(f\"  Atributo {idx}:  RMSE = {all_metrics['rmse'][i]:.4f},  MSE = {all_metrics['mse'][i]:.4f},  MAE = {all_metrics['mae'][i]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Evaluate AE reconstruction error\")\n",
    "parser.add_argument(\"--model\", required=True, help=\"Path to saved Keras model\")\n",
    "parser.add_argument(\"--data\", required=True, help=\"Path to .npy array\")\n",
    "parser.add_argument(\"--attrs\", nargs=\"+\", type=int, required=True,\n",
    "                        help=\"Indices of attributes to reconstruct\")\n",
    "parser.add_argument(\"--out\", default=\"reconstructed_attrs.npy\",\n",
    "                        help=\"File to save reconstructed attributes\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Load data\n",
    "data = np.load(args.data).astype(np.float32)\n",
    "metrics, recon_subset = reconstruct_and_evaluate(args.model, data, args.attrs, batch_size=32)\n",
    "\n",
    "np.save(args.out, recon_subset)\n",
    "\n",
    "print(\"Reconstruction error per attribute:\")\n",
    "for i, idx in enumerate(args.attrs):\n",
    "    print(f\"  Attr {idx}: RMSE={metrics['rmse'][i]:.6f} MSE={metrics['mse'][i]:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inv_Di",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
