{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2560fba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 14:10:55.934929: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-17 14:10:55.951469: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-17 14:10:55.956514: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-17 14:10:55.968530: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-17 14:10:56.925505: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed precision enabled\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys \n",
    "os.chdir('..')\n",
    "sys.path.insert(0, os.getcwd())\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K \n",
    "import glob\n",
    "\n",
    "from Data_loader import load_subjects_from_json, get_all_npy_paths_by_group, base_folders\n",
    "from AE_pipeline import (\n",
    "    convert_npy_to_tfrecord,\n",
    "    create_tfrecord_dataset,\n",
    "    write_sharded_tfrecord,\n",
    "    make_monolithic_ds,\n",
    "    build_lstm_autoencoder,\n",
    "    train_autoencoder,\n",
    "    evaluate_and_detect,\n",
    "    extract_and_save_latents,\n",
    "    n_timesteps,\n",
    "    NUM_BIOMECHANICAL_VARIABLES,\n",
    "    _parse_cycle,\n",
    "    BATCH_SIZE,\n",
    "    reconstruct_and_evaluate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af456de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load subjects lists \n",
    "train_subjects = {\n",
    "    \"G01\": load_subjects_from_json(\"G01_train_subjects.json\"),\n",
    "    \"G03\": load_subjects_from_json(\"G03_train_subjects.json\")\n",
    "}\n",
    "val_subjects = {\n",
    "    \"G01\": load_subjects_from_json(\"G01_validation_subjects.json\"),\n",
    "    \"G03\": load_subjects_from_json(\"G03_validation_subjects.json\")\n",
    "}\n",
    "test_subjects = {\n",
    "    \"G01\": load_subjects_from_json(\"G01_test_subjects.json\"),\n",
    "    \"G03\": load_subjects_from_json(\"G03_test_subjects.json\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "379bf567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S030/preprocessed/S030_D01_B01_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S030/preprocessed/S030_D01_B02_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S030/preprocessed/S030_D01_B02_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S030/preprocessed/S030_D01_B02_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S030/preprocessed/S030_D01_B03_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B01_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B01_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B01_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B02_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B02_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B02_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B03_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S017/preprocessed/S017_D02_B03_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S012/preprocessed/S012_D01_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S014/preprocessed/S014_D02_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S033/preprocessed/S033_D02_B01_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S033/preprocessed/S033_D02_B02_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S031/preprocessed/S031_D01_B01_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S031/preprocessed/S031_D02_B01_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S031/preprocessed/S031_D02_B02_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S031/preprocessed/S031_D02_B02_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S031/preprocessed/S031_D02_B03_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S031/preprocessed/S031_D02_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S039/preprocessed/S039_D01_B01_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S039/preprocessed/S039_D01_B02_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S039/preprocessed/S039_D01_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S034/preprocessed/S034_D01_B01_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S034/preprocessed/S034_D01_B02_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S034/preprocessed/S034_D01_B02_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S034/preprocessed/S034_D01_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B01_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B01_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B01_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B02_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B02_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B02_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B03_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S131/preprocessed/S131_D02_B03_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/young adults (19–35 years old)/S005/preprocessed/S005_D02_B01_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S019/preprocessed/S019_D01_B03_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S107/preprocessed/S107_D01_B03_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D01_B01_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B01_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B01_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B01_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B02_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B02_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B02_T03_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B03_T01_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B03_T02_preprocessed.npy\n",
      "Warning: missing file /mnt/storage/dmartinez/old adults (56+ years old)/S104/preprocessed/S104_D02_B03_T03_preprocessed.npy\n",
      "Train .npy: 932 files\n",
      " Val  .npy: 196 files\n",
      " Test .npy: 169 files\n"
     ]
    }
   ],
   "source": [
    "# Generate routes .npy\n",
    "train_npy = get_all_npy_paths_by_group(train_subjects, base_folders)\n",
    "val_npy   = get_all_npy_paths_by_group(val_subjects,   base_folders)\n",
    "test_npy  = get_all_npy_paths_by_group(test_subjects,  base_folders)\n",
    "\n",
    "print(f\"Train .npy: {len(train_npy)} files\")\n",
    "print(f\" Val  .npy: {len(val_npy)} files\")\n",
    "print(f\" Test .npy: {len(test_npy)} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0e7a263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Shards ya existen en: train_shards/\n"
     ]
    }
   ],
   "source": [
    "#Generar SHARDS para TRAIN \n",
    "shards_dir = \"train_shards\"\n",
    "if not os.path.isdir(shards_dir):\n",
    "    write_sharded_tfrecord(\n",
    "        npy_paths=train_npy,\n",
    "        output_dir=shards_dir,\n",
    "        shard_size=5_000\n",
    "    )\n",
    "    print(f\"→ Shards generados en: {shards_dir}/\")\n",
    "else:\n",
    "    print(f\"→ Shards ya existen en: {shards_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa0562cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping (already exists) → val_cycles.tfrecord.gz\n",
      "Skipping (already exists) → test_cycles.tfrecord.gz\n"
     ]
    }
   ],
   "source": [
    "#Convertir VAL y TEST a TFRecord monolítico \n",
    "for split, npy_list in [(\"val\", val_npy), (\"test\", test_npy)]:\n",
    "    tfp = f\"{split}_cycles.tfrecord.gz\"\n",
    "    if not os.path.exists(tfp):\n",
    "        convert_npy_to_tfrecord(npy_list, tfp)\n",
    "        print(f\"Converted → {tfp}\")\n",
    "    else:\n",
    "        print(f\"Skipping (already exists) → {tfp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ac1279c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1750180271.254533   20397 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750180271.293898   20397 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750180271.294108   20397 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750180271.295594   20397 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750180271.295845   20397 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750180271.295991   20397 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750180271.351034   20397 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750180271.351281   20397 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1750180271.351426   20397 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2025-06-17 14:11:11.351529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6854 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ train_ds: <_PrefetchDataset element_spec=(TensorSpec(shape=(256, 100, 321), dtype=tf.float32, name=None), TensorSpec(shape=(256, 100, 321), dtype=tf.float32, name=None))>\n",
      "→ val_ds:   <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 100, 321), dtype=tf.float32, name=None), TensorSpec(shape=(None, 100, 321), dtype=tf.float32, name=None))>\n",
      "→ test_ds:  <_PrefetchDataset element_spec=(TensorSpec(shape=(None, 100, 321), dtype=tf.float32, name=None), TensorSpec(shape=(None, 100, 321), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "#Create tf.data.Dataset\n",
    "# 3a) Lista de archivos shard\n",
    "shard_files = sorted(glob.glob(os.path.join(shards_dir, \"*.tfrecord.gz\")))\n",
    "\n",
    "# 3b) Pipeline shard-aware\n",
    "train_ds = (\n",
    "   tf.data.Dataset\n",
    "      .list_files(shard_files, shuffle=True)\n",
    "      .interleave(\n",
    "         lambda f: tf.data.TFRecordDataset(f, compression_type=\"GZIP\"),\n",
    "         cycle_length=4,\n",
    "         num_parallel_calls=tf.data.AUTOTUNE\n",
    "      )\n",
    "      .map(_parse_cycle, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "      .shuffle(5_000, seed=42)\n",
    "      .batch(BATCH_SIZE, drop_remainder=True)\n",
    "      .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# VAL y TEST: dataset monolítico\n",
    "\n",
    "\n",
    "val_ds  = make_monolithic_ds(\"val_cycles.tfrecord.gz\")\n",
    "test_ds = make_monolithic_ds(\"test_cycles.tfrecord.gz\")\n",
    "\n",
    "print(f\"→ train_ds: {train_ds}\")\n",
    "print(f\"→ val_ds:   {val_ds}\")\n",
    "print(f\"→ test_ds:  {test_ds}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f3d333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input  ◂ min: -12.165039 max: 15.267122 mean: -0.009751429 std: 1.0221982\n",
      "Target ◂ -12.165039 15.267122\n",
      "Any NaN in x? False\n",
      "Any Inf in x? False\n"
     ]
    }
   ],
   "source": [
    "# Optional from a batch validate if still NaN \n",
    "for x_batch, y_batch in train_ds.take(1):\n",
    "      import tensorflow as tf\n",
    "      print(\"Input  ◂ min:\", tf.reduce_min(x_batch).numpy(),\n",
    "            \"max:\", tf.reduce_max(x_batch).numpy(),\n",
    "            \"mean:\", tf.reduce_mean(x_batch).numpy(),\n",
    "            \"std:\", tf.math.reduce_std(x_batch).numpy())\n",
    "      print(\"Target ◂\", \n",
    "            tf.reduce_min(y_batch).numpy(), tf.reduce_max(y_batch).numpy())\n",
    "      # Comprueba si hay NaN/Inf\n",
    "      print(\"Any NaN in x?\", tf.reduce_any(tf.math.is_nan(x_batch)).numpy())\n",
    "      print(\"Any Inf in x?\", tf.reduce_any(tf.math.is_inf(x_batch)).numpy())\n",
    "      break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09373d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760\n",
      "165\n"
     ]
    }
   ],
   "source": [
    "#Optional Identify # cycles in train and validation \n",
    "import numpy as np, math\n",
    "# 1) Total de ciclos en train/val\n",
    "total_train_cycles = sum(np.load(p).shape[0] for p in train_npy)\n",
    "total_val_cycles   = sum(np.load(p).shape[0] for p in val_npy)\n",
    "print(total_train_cycles)\n",
    "print(total_val_cycles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional steps_per_epoch & validation_steps\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch    = total_train_cycles // BATCH_SIZE\n",
    "validation_steps   = total_val_cycles   // BATCH_SIZE\n",
    "print(steps_per_epoch)\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f0aef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m760/760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 284ms/step - loss: 1.2599 - r2: 0.0092 - root_mean_squared_error: 1.0231 - val_loss: 1.0869 - val_r2: 0.0626 - val_root_mean_squared_error: 1.0079 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 14:23:34.139692: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2025-06-17 14:23:34.139775: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n",
      "/home/dmartinez/miniconda3/envs/inv_Di/lib/python3.10/site-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m760/760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - loss: 0.0000e+00 - r2: 0.0000e+00 - root_mean_squared_error: 0.0000e+00 - val_loss: 1.0869 - val_r2: 0.0626 - val_root_mean_squared_error: 1.0079 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m760/760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 276ms/step - loss: 0.8848 - r2: 0.1501 - root_mean_squared_error: 0.9046 - val_loss: 0.8821 - val_r2: 0.2496 - val_root_mean_squared_error: 0.9074 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 14:27:33.617079: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m760/760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - loss: 0.0000e+00 - r2: 0.0000e+00 - root_mean_squared_error: 0.0000e+00 - val_loss: 0.8821 - val_r2: 0.2496 - val_root_mean_squared_error: 0.9074 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m760/760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 281ms/step - loss: 0.7796 - r2: 0.2895 - root_mean_squared_error: 0.8495 - val_loss: 0.8321 - val_r2: 0.2973 - val_root_mean_squared_error: 0.8807 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m760/760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - loss: 0.0000e+00 - r2: 0.0000e+00 - root_mean_squared_error: 0.0000e+00 - val_loss: 0.8321 - val_r2: 0.2973 - val_root_mean_squared_error: 0.8807 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m760/760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 283ms/step - loss: 0.7678 - r2: 0.3356 - root_mean_squared_error: 0.8420 - val_loss: 0.8078 - val_r2: 0.3203 - val_root_mean_squared_error: 0.8689 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 14:35:40.544900: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m760/760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - loss: 0.0000e+00 - r2: 0.0000e+00 - root_mean_squared_error: 0.0000e+00 - val_loss: 0.8078 - val_r2: 0.3203 - val_root_mean_squared_error: 0.8689 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m760/760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 284ms/step - loss: 0.7350 - r2: 0.3576 - root_mean_squared_error: 0.8244 - val_loss: 0.7964 - val_r2: 0.3289 - val_root_mean_squared_error: 0.8638 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m760/760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 37ms/step - loss: 0.0000e+00 - r2: 0.0000e+00 - root_mean_squared_error: 0.0000e+00 - val_loss: 0.7964 - val_r2: 0.3289 - val_root_mean_squared_error: 0.8638 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m 60/760\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:56\u001b[0m 253ms/step - loss: 0.6860 - r2: 0.3741 - root_mean_squared_error: 0.7971"
     ]
    }
   ],
   "source": [
    "#Build and train the Autoencoder\n",
    "# Hiperparameters \n",
    "def r2(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    ss_total = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    ss_residual = K.sum(K.square(y_true - y_pred))\n",
    "    return 1 - (ss_residual / ss_total)\n",
    "\n",
    "run_id = \"32_tanh_lr1e4_30ep_AdamW\"\n",
    "n_timesteps = 100\n",
    "n_vars = 321\n",
    "latent_dim = 32\n",
    "epochs = 50\n",
    "lr_initial   = 1e-4\n",
    "lr_decay_rate = 0.98\n",
    "lr_decay_steps = 5000\n",
    "clipnorm     = 1.0\n",
    "steps_per_epoch  = 760\n",
    "validation_steps = 165\n",
    "\n",
    "model = build_lstm_autoencoder(\n",
    "    n_timesteps=n_timesteps,\n",
    "    n_vars=n_vars,\n",
    "    latent_dim=latent_dim,\n",
    "    )\n",
    "\n",
    "history = train_autoencoder(\n",
    "    model=model,\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    run_id=run_id,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps\n",
    "    )\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'saved_models/best_ae_16_adamw_50ep.keras'  \n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5ec9966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 11:21:33.181680: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "/home/dmartinez/miniconda3/envs/inv_Di/lib/python3.10/site-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test reconstruction MSE: 0.583976\n",
      "Detected 4667 anomalies out of 35115 (threshold=0.871904)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 11:22:27.171544: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "all_losses, threshold = evaluate_and_detect(model, test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Latent characteristics \n",
    "latents = extract_and_save_latents(model, test_ds, output_path=\"latent_features_test.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b74e259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstruct from latent and measure error \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from reconstruct_module import reconstruct_and_evaluate  # ajusta al nombre del módulo\n",
    "\n",
    "# 1) Usamos test_ds en lugar de cargar un archivo .npy\n",
    "#  - Ya tienes test_ds previamente cargado, solo usas ese dataset\n",
    "\n",
    "# Define los índices de las variables que te interesan\n",
    "attrs = [0, 10, 50]\n",
    "\n",
    "# Crear un Dataset iterador para obtener los datos de test en batches\n",
    "# Usamos un batch size más pequeño para evaluar (si lo necesitas)\n",
    "batch_size = 32\n",
    "\n",
    "# Recorremos el dataset por lotes\n",
    "all_metrics = []\n",
    "all_recon = []\n",
    "\n",
    "for batch_data, _ in test_ds.take(-1):  # Recorre todo el test_ds\n",
    "    batch_data = batch_data.numpy()  # Convertir de Tensor a NumPy array si es necesario\n",
    "\n",
    "    # Llama a la función de reconstrucción y evaluación\n",
    "    metrics, recon_subset = reconstruct_and_evaluate(\n",
    "        model_path=\"saved_models/ae_lstm_experiment42.keras\",\n",
    "        data=batch_data,  # Aquí pasamos cada batch del test_ds\n",
    "        attr_idx=attrs,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    all_metrics.append(metrics)  # Guarda los resultados\n",
    "    all_recon.append(recon_subset)  # Guarda las reconstrucciones\n",
    "\n",
    "# Concatenar resultados de todas las épocas\n",
    "all_metrics = {key: np.concatenate([m[key] for m in all_metrics], axis=0) for key in all_metrics[0].keys()}\n",
    "all_recon = np.concatenate(all_recon, axis=0)\n",
    "\n",
    "# 2) Guarda la reconstrucción y muestra los errores\n",
    "np.save(\"reconstructed_attrs.npy\", all_recon)\n",
    "\n",
    "print(\"Reconstructions saved to reconstructed_attrs.npy\\n\")\n",
    "print(\"Error por atributo:\")\n",
    "for i, idx in enumerate(attrs):\n",
    "    print(f\"  Atributo {idx}:  RMSE = {all_metrics['rmse'][i]:.4f},  MSE = {all_metrics['mse'][i]:.4f},  MAE = {all_metrics['mae'][i]:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Evaluate AE reconstruction error\")\n",
    "parser.add_argument(\"--model\", required=True, help=\"Path to saved Keras model\")\n",
    "parser.add_argument(\"--data\", required=True, help=\"Path to .npy array\")\n",
    "parser.add_argument(\"--attrs\", nargs=\"+\", type=int, required=True,\n",
    "                        help=\"Indices of attributes to reconstruct\")\n",
    "parser.add_argument(\"--out\", default=\"reconstructed_attrs.npy\",\n",
    "                        help=\"File to save reconstructed attributes\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Load data\n",
    "data = np.load(args.data).astype(np.float32)\n",
    "metrics, recon_subset = reconstruct_and_evaluate(args.model, data, args.attrs, batch_size=32)\n",
    "\n",
    "np.save(args.out, recon_subset)\n",
    "\n",
    "print(\"Reconstruction error per attribute:\")\n",
    "for i, idx in enumerate(args.attrs):\n",
    "    print(f\"  Attr {idx}: RMSE={metrics['rmse'][i]:.6f} MSE={metrics['mse'][i]:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inv_Di",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
